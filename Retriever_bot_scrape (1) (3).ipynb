{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37406d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\almas\\anaconda3\\lib\\site-packages (4.26.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from selenium) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\almas\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\almas\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\almas\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\almas\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\almas\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\almas\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\almas\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\almas\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd339f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484db978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your ChromeDriver\n",
    "service = Service(executable_path=\"C:/Users/almas/Downloads/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7dbbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Global variables\n",
    "visited_links = set()\n",
    "results = []\n",
    "\n",
    "def extract_english_text(text):\n",
    "    \"\"\"\n",
    "    Extract only English words, numbers, and recognized English punctuation from the input text.\n",
    "    \"\"\"\n",
    "    # Match English words, numbers, and specific punctuation\n",
    "    pattern = r\"[A-Za-z0-9\\s.,!?\\\"'():;%-]\"\n",
    "    cleaned_text = ''.join(re.findall(pattern, text))\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Function to check if a page requires login or restricted access\n",
    "def requires_login_or_access_denied(driver):\n",
    "    \"\"\"Check if the page indicates login or restricted access is required.\"\"\"\n",
    "    page_source = driver.page_source.lower()\n",
    "    login_indicators = [\n",
    "        \"login\", \"access denied\", \"restricted access\", \"please sign in\",\n",
    "        \"requires login\", \"permission required\", \"sign in to continue\"\n",
    "    ]\n",
    "    return any(indicator in page_source for indicator in login_indicators)\n",
    "\n",
    "# Recursive function to scrape links within the \"Courses\" section\n",
    "def scrape_courses_section(driver, url, section_name, current_depth=1, max_depth=3):\n",
    "    \"\"\"Scrape links within a section up to a specified depth.\"\"\"\n",
    "    if url in visited_links or current_depth > max_depth:\n",
    "        return\n",
    "    visited_links.add(url)\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        # Check for restricted access\n",
    "        if requires_login_or_access_denied(driver):\n",
    "            results.append({\n",
    "                'Section': section_name,\n",
    "                'Link': url,\n",
    "                'Title': 'Login Required',\n",
    "                'Text': 'Requires login to access this content.'\n",
    "            })\n",
    "            return\n",
    "\n",
    "        # Extract main content\n",
    "        main_content_element = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'main-content')))\n",
    "        header_element = main_content_element.find_element(By.CLASS_NAME, 'entry-header')\n",
    "        title = extract_english_text(header_element.text)\n",
    "\n",
    "        content_element = main_content_element.find_element(By.CLASS_NAME, 'entry-content')\n",
    "        content = extract_english_text(content_element.text)\n",
    "\n",
    "        results.append({\n",
    "            'Section': section_name,\n",
    "            'Link': url,\n",
    "            'Title': title,\n",
    "            'Text': content\n",
    "        })\n",
    "\n",
    "        # Recursively visit internal links\n",
    "        links_in_content = content_element.find_elements(By.TAG_NAME, 'a')\n",
    "        internal_links = [link.get_attribute('href') for link in links_in_content if link.get_attribute('href')]\n",
    "\n",
    "        for link in internal_links:\n",
    "            if link and link not in visited_links:\n",
    "                scrape_courses_section(driver, link, section_name, current_depth + 1, max_depth)\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "# Function to extract main content\n",
    "def extract_main_content(url, section_name):\n",
    "    \"\"\"Extract and clean title and text from the main content.\"\"\"\n",
    "    if url in visited_links:\n",
    "        return None\n",
    "    visited_links.add(url)\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        main_content = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'main-content')))\n",
    "        entry_header = main_content.find_element(By.CLASS_NAME, 'entry-header')\n",
    "        title = extract_english_text(entry_header.find_element(By.CLASS_NAME, 'entry-title').text)\n",
    "\n",
    "        entry_content = main_content.find_element(By.CLASS_NAME, 'entry-content')\n",
    "        general_text = extract_english_text(entry_content.get_attribute('innerText'))\n",
    "\n",
    "        results.append({\n",
    "            'Section': section_name,\n",
    "            'Link': url,\n",
    "            'Title': title,\n",
    "            'Text': general_text\n",
    "        })\n",
    "        return main_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting main content from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_internal_links(main_content, section_name):\n",
    "    \"\"\"Identify and visit internal links from the main content.\"\"\"\n",
    "    try:\n",
    "        # Find all anchor tags within the entry-content div\n",
    "        anchor_tags = main_content.find_elements(By.TAG_NAME, 'a')\n",
    "        links_to_visit = []\n",
    "\n",
    "        for anchor in anchor_tags:\n",
    "            href = anchor.get_attribute('href')\n",
    "            if href and \"dil.umbc.edu\" in href and href not in visited_links:\n",
    "                links_to_visit.append(href)\n",
    "\n",
    "        # Visit each link and extract data\n",
    "        for link in links_to_visit:\n",
    "            main_content = extract_main_content(link, section_name)\n",
    "            if main_content:\n",
    "                extract_collapsible_sections(main_content, link, section_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding internal links: {e}\")\n",
    "\n",
    "def extract_main_content_notitle(url, section_name):\n",
    "    \"\"\"Extract title and text from the entry-header, and detailed subheading content within entry-content.\"\"\"\n",
    "    if url in visited_links:\n",
    "        return None  # Skip if the URL has already been visited\n",
    "    visited_links.add(url)\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Wait for the main content to load\n",
    "        main_content = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'main-content')))\n",
    "        \n",
    "\n",
    "        # Extract general text from entry-content\n",
    "        entry_content = main_content.find_element(By.CLASS_NAME, 'entry-content')\n",
    "        general_text = entry_content.get_attribute('innerText').strip()\n",
    "        \n",
    "        # Store the main title and general text for the section\n",
    "        results.append({\n",
    "            'Section': section_name,\n",
    "            'Link': url,\n",
    "            'Title': f\"{section_name} - Main Content\",\n",
    "            'Text': general_text\n",
    "        })\n",
    "\n",
    "        return main_content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting main content from {url}: {e}\")\n",
    "        return None  \n",
    "# Function to extract collapsible sections\n",
    "def extract_collapsible_sections(main_content, url, section_name):\n",
    "    \"\"\"Extract collapsible sections and their content within the main content.\"\"\"\n",
    "    sections = main_content.find_elements(By.CLASS_NAME, 'sights-expander-wrapper')\n",
    "    for section in sections:\n",
    "        try:\n",
    "            title_element = section.find_element(By.TAG_NAME, 'h5')\n",
    "            title = extract_english_text(title_element.text)\n",
    "\n",
    "            expander_trigger = section.find_element(By.CLASS_NAME, 'sights-expander-trigger')\n",
    "            expander_trigger.click()\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.visibility_of(section.find_element(By.CLASS_NAME, 'sights-expander-content'))\n",
    "            )\n",
    "\n",
    "            content_div = section.find_element(By.CLASS_NAME, 'sights-expander-content')\n",
    "            text = extract_english_text(content_div.text)\n",
    "\n",
    "            results.append({\n",
    "                'Section': section_name,\n",
    "                'Link': url,\n",
    "                'Title': title,\n",
    "                'Text': text\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting collapsible section from {url}: {e}\")\n",
    "\n",
    "# Function to scrape links from the sidebar\n",
    "def scrape_sidebar_links(url, section_name):\n",
    "    \"\"\"Scrape all links found in the sidebar and extract content.\"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        sidebar_links = driver.find_elements(By.CSS_SELECTOR, '.sidebar a')\n",
    "        for link in sidebar_links:\n",
    "            href = link.get_attribute('href')\n",
    "            if href and href not in visited_links:\n",
    "                extract_main_content(href, section_name)\n",
    "                time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting sidebar links from {url}: {e}\")\n",
    "\n",
    "# Function to extract dropdown content\n",
    "def extract_dropdown_content(url, section_name):\n",
    "    \"\"\"Extract content from dropdown sections on the page.\"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        dropdown_sections = driver.find_elements(By.CLASS_NAME, 'sights-expander-wrapper')\n",
    "        for section in dropdown_sections:\n",
    "            try:\n",
    "                question_element = section.find_element(By.CLASS_NAME, 'mceEditable')\n",
    "                question_text = extract_english_text(question_element.text)\n",
    "\n",
    "                expander_trigger = section.find_element(By.CLASS_NAME, 'sights-expander-trigger')\n",
    "                driver.execute_script(\"arguments[0].click();\", expander_trigger)\n",
    "                time.sleep(1)\n",
    "\n",
    "                answer_id = expander_trigger.get_attribute('aria-controls')\n",
    "                answer_content = WebDriverWait(driver, 5).until(\n",
    "                    EC.visibility_of_element_located((By.ID, answer_id))\n",
    "                )\n",
    "                answer_text = extract_english_text(answer_content.text)\n",
    "\n",
    "                results.append({\n",
    "                    'Section': section_name,\n",
    "                    'Link': url,\n",
    "                    'Title': question_text,\n",
    "                    'Text': answer_text\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting dropdown content in {url}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dropdown sections from {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf03989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping section: Courses\n",
      "Error scraping https://professionalprograms.umbc.edu/data-science/masters-of-professional-studies-data-science/: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7FD3EE1F5+2972373]\n",
      "\t(No symbol) [0x00007FF7FD0855F0]\n",
      "\t(No symbol) [0x00007FF7FCF257FA]\n",
      "\t(No symbol) [0x00007FF7FCF75A3E]\n",
      "\t(No symbol) [0x00007FF7FCF75D2C]\n",
      "\t(No symbol) [0x00007FF7FCFBEAB7]\n",
      "\t(No symbol) [0x00007FF7FCF9BABF]\n",
      "\t(No symbol) [0x00007FF7FCFBB8CC]\n",
      "\t(No symbol) [0x00007FF7FCF9B823]\n",
      "\t(No symbol) [0x00007FF7FCF675E8]\n",
      "\t(No symbol) [0x00007FF7FCF68751]\n",
      "\tGetHandleVerifier [0x00007FF7FD4147BD+3129501]\n",
      "\tGetHandleVerifier [0x00007FF7FD464D00+3458528]\n",
      "\tGetHandleVerifier [0x00007FF7FD45B05D+3418429]\n",
      "\tGetHandleVerifier [0x00007FF7FD1E687B+844123]\n",
      "\t(No symbol) [0x00007FF7FD090AFF]\n",
      "\t(No symbol) [0x00007FF7FD08C6D4]\n",
      "\t(No symbol) [0x00007FF7FD08C86D]\n",
      "\t(No symbol) [0x00007FF7FD07BD79]\n",
      "\tBaseThreadInitThunk [0x00007FFECCF2259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFECE6AAF38+40]\n",
      "\n",
      "Scraping section: Pathways & Certificates\n",
      "Error extracting sidebar links from https://dil.umbc.edu/pathways-and-certificates/: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=130.0.6723.117); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7FD3EE1F5+2972373]\n",
      "\t(No symbol) [0x00007FF7FD0855F0]\n",
      "\t(No symbol) [0x00007FF7FCF257FA]\n",
      "\t(No symbol) [0x00007FF7FCF37CDB]\n",
      "\t(No symbol) [0x00007FF7FCF2CF04]\n",
      "\t(No symbol) [0x00007FF7FCF2D089]\n",
      "\t(No symbol) [0x00007FF7FCF2AE09]\n",
      "\t(No symbol) [0x00007FF7FCF2E8FF]\n",
      "\t(No symbol) [0x00007FF7FCFBCBB0]\n",
      "\t(No symbol) [0x00007FF7FCF9BA7A]\n",
      "\t(No symbol) [0x00007FF7FCFBB8CC]\n",
      "\t(No symbol) [0x00007FF7FCF9B823]\n",
      "\t(No symbol) [0x00007FF7FCF675E8]\n",
      "\t(No symbol) [0x00007FF7FCF68751]\n",
      "\tGetHandleVerifier [0x00007FF7FD4147BD+3129501]\n",
      "\tGetHandleVerifier [0x00007FF7FD464D00+3458528]\n",
      "\tGetHandleVerifier [0x00007FF7FD45B05D+3418429]\n",
      "\tGetHandleVerifier [0x00007FF7FD1E687B+844123]\n",
      "\t(No symbol) [0x00007FF7FD090AFF]\n",
      "\t(No symbol) [0x00007FF7FD08C6D4]\n",
      "\t(No symbol) [0x00007FF7FD08C86D]\n",
      "\t(No symbol) [0x00007FF7FD07BD79]\n",
      "\tBaseThreadInitThunk [0x00007FFECCF2259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFECE6AAF38+40]\n",
      "\n",
      "Scraping section: Advising & Resources\n",
      "Error extracting sidebar links from https://dil.umbc.edu/resources/: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=130.0.6723.117); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7FD3EE1F5+2972373]\n",
      "\t(No symbol) [0x00007FF7FD0855F0]\n",
      "\t(No symbol) [0x00007FF7FCF257FA]\n",
      "\t(No symbol) [0x00007FF7FCF37CDB]\n",
      "\t(No symbol) [0x00007FF7FCF2CF04]\n",
      "\t(No symbol) [0x00007FF7FCF2D089]\n",
      "\t(No symbol) [0x00007FF7FCF2AE09]\n",
      "\t(No symbol) [0x00007FF7FCF2E8FF]\n",
      "\t(No symbol) [0x00007FF7FCFBCBB0]\n",
      "\t(No symbol) [0x00007FF7FCF9BA7A]\n",
      "\t(No symbol) [0x00007FF7FCFBB8CC]\n",
      "\t(No symbol) [0x00007FF7FCF9B823]\n",
      "\t(No symbol) [0x00007FF7FCF675E8]\n",
      "\t(No symbol) [0x00007FF7FCF68751]\n",
      "\tGetHandleVerifier [0x00007FF7FD4147BD+3129501]\n",
      "\tGetHandleVerifier [0x00007FF7FD464D00+3458528]\n",
      "\tGetHandleVerifier [0x00007FF7FD45B05D+3418429]\n",
      "\tGetHandleVerifier [0x00007FF7FD1E687B+844123]\n",
      "\t(No symbol) [0x00007FF7FD090AFF]\n",
      "\t(No symbol) [0x00007FF7FD08C6D4]\n",
      "\t(No symbol) [0x00007FF7FD08C86D]\n",
      "\t(No symbol) [0x00007FF7FD07BD79]\n",
      "\tBaseThreadInitThunk [0x00007FFECCF2259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFECE6AAF38+40]\n",
      "\n",
      "Scraping section: Policies\n",
      "Error extracting sidebar links from https://dil.umbc.edu/policies/: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=130.0.6723.117); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7FD3EE1F5+2972373]\n",
      "\t(No symbol) [0x00007FF7FD0855F0]\n",
      "\t(No symbol) [0x00007FF7FCF257FA]\n",
      "\t(No symbol) [0x00007FF7FCF37CDB]\n",
      "\t(No symbol) [0x00007FF7FCF2CF04]\n",
      "\t(No symbol) [0x00007FF7FCF2D089]\n",
      "\t(No symbol) [0x00007FF7FCF2AE09]\n",
      "\t(No symbol) [0x00007FF7FCF2E8FF]\n",
      "\t(No symbol) [0x00007FF7FCFBCBB0]\n",
      "\t(No symbol) [0x00007FF7FCF9BA7A]\n",
      "\t(No symbol) [0x00007FF7FCFBB8CC]\n",
      "\t(No symbol) [0x00007FF7FCF9B823]\n",
      "\t(No symbol) [0x00007FF7FCF675E8]\n",
      "\t(No symbol) [0x00007FF7FCF68751]\n",
      "\tGetHandleVerifier [0x00007FF7FD4147BD+3129501]\n",
      "\tGetHandleVerifier [0x00007FF7FD464D00+3458528]\n",
      "\tGetHandleVerifier [0x00007FF7FD45B05D+3418429]\n",
      "\tGetHandleVerifier [0x00007FF7FD1E687B+844123]\n",
      "\t(No symbol) [0x00007FF7FD090AFF]\n",
      "\t(No symbol) [0x00007FF7FD08C6D4]\n",
      "\t(No symbol) [0x00007FF7FD08C86D]\n",
      "\t(No symbol) [0x00007FF7FD07BD79]\n",
      "\tBaseThreadInitThunk [0x00007FFECCF2259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFECE6AAF38+40]\n",
      "\n",
      "Scraping section: Prospective Students\n",
      "Error extracting sidebar links from https://dil.umbc.edu/prospective-students/: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=130.0.6723.117); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7FD3EE1F5+2972373]\n",
      "\t(No symbol) [0x00007FF7FD0855F0]\n",
      "\t(No symbol) [0x00007FF7FCF257FA]\n",
      "\t(No symbol) [0x00007FF7FCF37CDB]\n",
      "\t(No symbol) [0x00007FF7FCF2CF04]\n",
      "\t(No symbol) [0x00007FF7FCF2D089]\n",
      "\t(No symbol) [0x00007FF7FCF2AE09]\n",
      "\t(No symbol) [0x00007FF7FCF2E8FF]\n",
      "\t(No symbol) [0x00007FF7FCFBCBB0]\n",
      "\t(No symbol) [0x00007FF7FCF9BA7A]\n",
      "\t(No symbol) [0x00007FF7FCFBB8CC]\n",
      "\t(No symbol) [0x00007FF7FCF9B823]\n",
      "\t(No symbol) [0x00007FF7FCF675E8]\n",
      "\t(No symbol) [0x00007FF7FCF68751]\n",
      "\tGetHandleVerifier [0x00007FF7FD4147BD+3129501]\n",
      "\tGetHandleVerifier [0x00007FF7FD464D00+3458528]\n",
      "\tGetHandleVerifier [0x00007FF7FD45B05D+3418429]\n",
      "\tGetHandleVerifier [0x00007FF7FD1E687B+844123]\n",
      "\t(No symbol) [0x00007FF7FD090AFF]\n",
      "\t(No symbol) [0x00007FF7FD08C6D4]\n",
      "\t(No symbol) [0x00007FF7FD08C86D]\n",
      "\t(No symbol) [0x00007FF7FD07BD79]\n",
      "\tBaseThreadInitThunk [0x00007FFECCF2259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFECE6AAF38+40]\n",
      "\n",
      "Scraping section: Faculty\n"
     ]
    }
   ],
   "source": [
    "# # Set up Chrome options\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--no-sandbox\")\n",
    "# chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "# # chrome_options.add_argument(\"--headless\")  # Uncomment to run in headless mode\n",
    "\n",
    "# # Initialize the WebDriver\n",
    "# driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# # List of main sections with URLs\n",
    "# sections = {\n",
    "#     \"Courses\": 'https://dil.umbc.edu/courses/',\n",
    "#     \"Pathways & Certificates\": 'https://dil.umbc.edu/pathways-and-certificates/',\n",
    "#     \"Advising & Resources\": 'https://dil.umbc.edu/resources/',\n",
    "#     \"Policies\": 'https://dil.umbc.edu/policies/',\n",
    "#     \"Prospective Students\": 'https://dil.umbc.edu/prospective-students/',\n",
    "#     \"Faculty\": 'https://dil.umbc.edu/faculty/'\n",
    "# }\n",
    "\n",
    "# # Initialize a list to store the results\n",
    "# results = []\n",
    "# visited_links = set()\n",
    "\n",
    "# # Main scraping process for each section\n",
    "# try:\n",
    "#     for section_name, url in sections.items():\n",
    "#         print(f\"Scraping section: {section_name}\")\n",
    "        \n",
    "#         if section_name == \"Courses\":\n",
    "#             scrape_courses_section(driver, url,section_name)\n",
    "\n",
    "#         # Step 1: Extract content from the main page\n",
    "#         main_content = extract_main_content(url, section_name)\n",
    "\n",
    "#         # Step 2: Extract collapsible sections (if any)\n",
    "#         if main_content:\n",
    "#             extract_collapsible_sections(main_content, url, section_name)\n",
    "\n",
    "#             # Step 3: Identify and visit internal links within the main content\n",
    "#             scrape_internal_links(main_content, section_name)\n",
    "\n",
    "#             # Step 4: Scrape all links in the sidebar\n",
    "#             scrape_sidebar_links(url, section_name)\n",
    "\n",
    "# finally:\n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected data and reorder columns\n",
    "df_dil = pd.DataFrame(results, columns=['Section', 'Link', 'Title', 'Text'])\n",
    "\n",
    "# Save the DataFrame to an CSV file\n",
    "df_dil.to_csv('C:/Users/almas/OneDrive/Desktop/Fall 2024/webscraping/dil_scraped_data.csv', index=False)\n",
    "\n",
    "\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"C:/Users/almas/OneDrive/Desktop/Fall 2024/webscraping/scraping_log.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def log_update_date(message):\n",
    "    \"\"\"Log a custom message with the current date.\"\"\"\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    logging.info(f\"{message} - Date: {current_date}\")\n",
    "\n",
    "# Scraping logic with integrated logging\n",
    "try:\n",
    "    log_update_date(\"Scraping started\")\n",
    "\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    # chrome_options.add_argument(\"--headless\")  # Uncomment to run in headless mode\n",
    "    \n",
    "    # Initialize the WebDriver\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    # List of main sections with URLs\n",
    "    sections = {\n",
    "        \"Courses\": 'https://dil.umbc.edu/courses/',\n",
    "        \"Pathways & Certificates\": 'https://dil.umbc.edu/pathways-and-certificates/',\n",
    "        \"Advising & Resources\": 'https://dil.umbc.edu/resources/',\n",
    "        \"Policies\": 'https://dil.umbc.edu/policies/',\n",
    "        \"Prospective Students\": 'https://dil.umbc.edu/prospective-students/',\n",
    "        \"Faculty\": 'https://dil.umbc.edu/faculty/'\n",
    "    }\n",
    "    \n",
    "    # Initialize a list to store the results\n",
    "    results = []\n",
    "    visited_links = set()\n",
    "    \n",
    "    # Main scraping process for each section\n",
    "    try:\n",
    "        for section_name, url in sections.items():\n",
    "            print(f\"Scraping section: {section_name}\")\n",
    "            \n",
    "            if section_name == \"Courses\":\n",
    "                scrape_courses_section(driver, url,section_name)\n",
    "    \n",
    "            # Step 1: Extract content from the main page\n",
    "            main_content = extract_main_content(url, section_name)\n",
    "    \n",
    "            # Step 2: Extract collapsible sections (if any)\n",
    "            if main_content:\n",
    "                extract_collapsible_sections(main_content, url, section_name)\n",
    "    \n",
    "                # Step 3: Identify and visit internal links within the main content\n",
    "                scrape_internal_links(main_content, section_name)\n",
    "    \n",
    "                # Step 4: Scrape all links in the sidebar\n",
    "                scrape_sidebar_links(url, section_name)\n",
    "    \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "    log_update_date(\"Scraping completed\")\n",
    "\n",
    "finally:\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df_dil = pd.DataFrame(results, columns=['Section', 'Link', 'Title', 'Text'])\n",
    "    df_dil.to_csv('C:/Users/almas/OneDrive/Desktop/Fall 2024/webscraping/dil_scraped_data.csv', index=False)\n",
    "    logging.info(\"CSV saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c9db146-22c1-49ab-8da4-b30030e99562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dil.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7b58b8b-163a-4164-99d8-d0c785edf20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Courses</td>\n",
       "      <td>https://dil.umbc.edu/courses/</td>\n",
       "      <td>Courses</td>\n",
       "      <td>Where can I find the\\nSyllabi of the Data Scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Courses</td>\n",
       "      <td>https://dil.umbc.edu/syllabi-of-data-science-c...</td>\n",
       "      <td>Syllabi of Data Science Courses</td>\n",
       "      <td>Core Data Science Courses\\nAll the masters stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Courses</td>\n",
       "      <td>https://dil.umbc.edu/courses/data-601-introduc...</td>\n",
       "      <td>DATA 601 Introduction to Data Science</td>\n",
       "      <td>Description: The goal of this class is to give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Courses</td>\n",
       "      <td>https://dil.umbc.edu/courses/data-602-introduc...</td>\n",
       "      <td>DATA 602 Introduction to Data Analysis and Mac...</td>\n",
       "      <td>Description: This course provides a broad intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Courses</td>\n",
       "      <td>https://dil.umbc.edu/courses/data-603-platform...</td>\n",
       "      <td>DATA 603 Platforms for Big Data Processing</td>\n",
       "      <td>Description: The goal of this course is to int...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Section                                               Link  \\\n",
       "0  Courses                      https://dil.umbc.edu/courses/   \n",
       "1  Courses  https://dil.umbc.edu/syllabi-of-data-science-c...   \n",
       "2  Courses  https://dil.umbc.edu/courses/data-601-introduc...   \n",
       "3  Courses  https://dil.umbc.edu/courses/data-602-introduc...   \n",
       "4  Courses  https://dil.umbc.edu/courses/data-603-platform...   \n",
       "\n",
       "                                               Title  \\\n",
       "0                                            Courses   \n",
       "1                    Syllabi of Data Science Courses   \n",
       "2              DATA 601 Introduction to Data Science   \n",
       "3  DATA 602 Introduction to Data Analysis and Mac...   \n",
       "4         DATA 603 Platforms for Big Data Processing   \n",
       "\n",
       "                                                Text  \n",
       "0  Where can I find the\\nSyllabi of the Data Scie...  \n",
       "1  Core Data Science Courses\\nAll the masters stu...  \n",
       "2  Description: The goal of this class is to give...  \n",
       "3  Description: This course provides a broad intr...  \n",
       "4  Description: The goal of this course is to int...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dil.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78dd9a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting main content from https://isss.umbc.edu/international-students-f-1/current-students-employment/working-off-campus/: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".entry-content\"}\n",
      "  (Session info: chrome=130.0.6723.117); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7FD3EE1F5+2972373]\n",
      "\t(No symbol) [0x00007FF7FD0855F0]\n",
      "\t(No symbol) [0x00007FF7FCF257FA]\n",
      "\t(No symbol) [0x00007FF7FCF75A3E]\n",
      "\t(No symbol) [0x00007FF7FCF75D2C]\n",
      "\t(No symbol) [0x00007FF7FCF6939C]\n",
      "\t(No symbol) [0x00007FF7FCF9BABF]\n",
      "\t(No symbol) [0x00007FF7FCF69266]\n",
      "\t(No symbol) [0x00007FF7FCF9BC90]\n",
      "\t(No symbol) [0x00007FF7FCFBB8CC]\n",
      "\t(No symbol) [0x00007FF7FCF9B823]\n",
      "\t(No symbol) [0x00007FF7FCF675E8]\n",
      "\t(No symbol) [0x00007FF7FCF68751]\n",
      "\tGetHandleVerifier [0x00007FF7FD4147BD+3129501]\n",
      "\tGetHandleVerifier [0x00007FF7FD464D00+3458528]\n",
      "\tGetHandleVerifier [0x00007FF7FD45B05D+3418429]\n",
      "\tGetHandleVerifier [0x00007FF7FD1E687B+844123]\n",
      "\t(No symbol) [0x00007FF7FD090AFF]\n",
      "\t(No symbol) [0x00007FF7FD08C6D4]\n",
      "\t(No symbol) [0x00007FF7FD08C86D]\n",
      "\t(No symbol) [0x00007FF7FD07BD79]\n",
      "\tBaseThreadInitThunk [0x00007FFECCF2259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFECE6AAF38+40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"C:/Users/almas/OneDrive/Desktop/Fall 2024/webscraping/scraping_isss_log.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def log_update_date(message):\n",
    "    \"\"\"Log a custom message with the current date.\"\"\"\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    logging.info(f\"{message} - Date: {current_date}\")\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment to run in headless mode\n",
    "\n",
    "# Initialize the WebDriver\n",
    "logging.info(\"Initializing WebDriver\")\n",
    "# service = Service('path/to/chromedriver')  # Replace with the correct path\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "logging.info(\"WebDriver initialized successfully\")\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "visited_links = set()\n",
    "\n",
    "# Define the F-1 Students sub-sections with their URLs\n",
    "sub_sections = {\n",
    "    \"Current Students: General\": \"https://isss.umbc.edu/international-students-f-1/current-students/\",\n",
    "    \"Current Students: Employment\": \"https://isss.umbc.edu/f-1-students/current-students-employment/\",\n",
    "    \"Working On-Campus\":\"https://isss.umbc.edu/international-students-f-1/current-students-employment/working-on-campus/\",\n",
    "    \"Economoic Hardship Work Authorization\":\"https://isss.umbc.edu/international-students-f-1/current-students-employment/economic-hardship-work-authorization/\",\n",
    "    \"Understanding Your Documents\": \"https://isss.umbc.edu/f-1-students/understanding-your-documents/\",\n",
    "    \"Social Security Number (SSN)\": \"https://isss.umbc.edu/f-1-students/social-security-number/\",\n",
    "    \"US Taxes\": \"https://isss.umbc.edu/international-students-f-1/current-students/understanding-your-tax-documents/\",\n",
    "    \"Applying for a Maryland Driverâ€™s License & Getting a State ID\": \"https://isss.umbc.edu/resources/transportation/driving-in-maryland-and-getting-a-state-id/\",\n",
    "    \"Change of Immigration Status\": \"https://isss.umbc.edu/change-of-status/\"\n",
    "}\n",
    "\n",
    "# List of URLs with dropdowns\n",
    "dropdown_pages = {\n",
    "    \"Internships and International Students\": \"https://dil.umbc.edu/resources/internships-and-international-students/\",\n",
    "    \"OPT and OPT STEM Information\": \"https://isss.umbc.edu/opt-and-opt-stem-information/\",\n",
    "    \"Working Off-Campus\": \"https://isss.umbc.edu/international-students-f-1/current-students-employment/working-off-campus/\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Scrape each specified sub-section and its sidebar links\n",
    "    for sub_section_name, link in sub_sections.items():\n",
    "        log_update_date(f\"Scraping started for section: {sub_section_name}\")\n",
    "        \n",
    "        # Step 1: Extract main content\n",
    "        main_content = extract_main_content_notitle(link, sub_section_name)\n",
    "        \n",
    "        if main_content:\n",
    "            logging.info(f\"Successfully scraped main content for section: {sub_section_name}\")\n",
    "        else:\n",
    "            logging.warning(f\"No main content found for section: {sub_section_name}\")\n",
    "\n",
    "    # Scrape each page with dropdowns\n",
    "    for section_name, url in dropdown_pages.items():\n",
    "        log_update_date(f\"Scraping dropdown section: {section_name}\")\n",
    "        \n",
    "        # Step 1: Extract content from the main page\n",
    "        main_content = extract_main_content_notitle(url, section_name)\n",
    "        \n",
    "        # Step 2: Extract collapsible sections (if any)\n",
    "        if main_content:\n",
    "            extract_dropdown_content(url, section_name)\n",
    "            logging.info(f\"Successfully extracted dropdown content for section: {section_name}\")\n",
    "        else:\n",
    "            logging.warning(f\"No dropdown content found for section: {section_name}\")\n",
    "\n",
    "finally:\n",
    "    logging.info(\"Closing WebDriver\")\n",
    "    driver.quit()\n",
    "    logging.info(\"WebDriver closed successfully\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_isss = pd.DataFrame(results, columns=['Section', 'Link', 'Title', 'Text'])\n",
    "output_file = 'C:/Users/almas/OneDrive/Desktop/Fall 2024/webscraping/isss_scraped_data.csv'\n",
    "df_isss.to_csv(output_file, index=False)\n",
    "logging.info(f\"Data saved to CSV file: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b57434cb-208a-4939-a52f-35478a04bf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_isss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "924fc067-e672-4fed-a055-8ea5cefa10e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Current Students: General</td>\n",
       "      <td>https://isss.umbc.edu/international-students-f...</td>\n",
       "      <td>Current Students: General - Main Content</td>\n",
       "      <td>In this section you will find more information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Current Students: Employment</td>\n",
       "      <td>https://isss.umbc.edu/f-1-students/current-stu...</td>\n",
       "      <td>Current Students: Employment - Main Content</td>\n",
       "      <td>International students have very specific kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Working On-Campus</td>\n",
       "      <td>https://isss.umbc.edu/international-students-f...</td>\n",
       "      <td>Working On-Campus - Main Content</td>\n",
       "      <td>F-1 International students are able to work up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Economoic Hardship Work Authorization</td>\n",
       "      <td>https://isss.umbc.edu/international-students-f...</td>\n",
       "      <td>Economoic Hardship Work Authorization - Main C...</td>\n",
       "      <td>If you are an F-1 student experiencing severe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Understanding Your Documents</td>\n",
       "      <td>https://isss.umbc.edu/f-1-students/understandi...</td>\n",
       "      <td>Understanding Your Documents - Main Content</td>\n",
       "      <td>While you are a student in the US there are a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Section  \\\n",
       "0              Current Students: General   \n",
       "1           Current Students: Employment   \n",
       "2                      Working On-Campus   \n",
       "3  Economoic Hardship Work Authorization   \n",
       "4           Understanding Your Documents   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://isss.umbc.edu/international-students-f...   \n",
       "1  https://isss.umbc.edu/f-1-students/current-stu...   \n",
       "2  https://isss.umbc.edu/international-students-f...   \n",
       "3  https://isss.umbc.edu/international-students-f...   \n",
       "4  https://isss.umbc.edu/f-1-students/understandi...   \n",
       "\n",
       "                                               Title  \\\n",
       "0           Current Students: General - Main Content   \n",
       "1        Current Students: Employment - Main Content   \n",
       "2                   Working On-Campus - Main Content   \n",
       "3  Economoic Hardship Work Authorization - Main C...   \n",
       "4        Understanding Your Documents - Main Content   \n",
       "\n",
       "                                                Text  \n",
       "0  In this section you will find more information...  \n",
       "1  International students have very specific kind...  \n",
       "2  F-1 International students are able to work up...  \n",
       "3  If you are an F-1 student experiencing severe ...  \n",
       "4  While you are a student in the US there are a ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_isss.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ece8a728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting sidebar links from https://www.csee.umbc.edu/csee-research-areas/: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=130.0.6723.117); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7FD3EE1F5+2972373]\n",
      "\t(No symbol) [0x00007FF7FD0855F0]\n",
      "\t(No symbol) [0x00007FF7FCF257FA]\n",
      "\t(No symbol) [0x00007FF7FCF37CDB]\n",
      "\t(No symbol) [0x00007FF7FCF2CF04]\n",
      "\t(No symbol) [0x00007FF7FCF2D089]\n",
      "\t(No symbol) [0x00007FF7FCF2AE09]\n",
      "\t(No symbol) [0x00007FF7FCF2E8FF]\n",
      "\t(No symbol) [0x00007FF7FCFBCBB0]\n",
      "\t(No symbol) [0x00007FF7FCF9BA7A]\n",
      "\t(No symbol) [0x00007FF7FCFBB8CC]\n",
      "\t(No symbol) [0x00007FF7FCF9B823]\n",
      "\t(No symbol) [0x00007FF7FCF675E8]\n",
      "\t(No symbol) [0x00007FF7FCF68751]\n",
      "\tGetHandleVerifier [0x00007FF7FD4147BD+3129501]\n",
      "\tGetHandleVerifier [0x00007FF7FD464D00+3458528]\n",
      "\tGetHandleVerifier [0x00007FF7FD45B05D+3418429]\n",
      "\tGetHandleVerifier [0x00007FF7FD1E687B+844123]\n",
      "\t(No symbol) [0x00007FF7FD090AFF]\n",
      "\t(No symbol) [0x00007FF7FD08C6D4]\n",
      "\t(No symbol) [0x00007FF7FD08C86D]\n",
      "\t(No symbol) [0x00007FF7FD07BD79]\n",
      "\tBaseThreadInitThunk [0x00007FFECCF2259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFECE6AAF38+40]\n",
      "\n",
      "Error extracting sidebar links from https://www.csee.umbc.edu/research-focus-areas-and-centers/: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=130.0.6723.117); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7FD3EE1F5+2972373]\n",
      "\t(No symbol) [0x00007FF7FD0855F0]\n",
      "\t(No symbol) [0x00007FF7FCF257FA]\n",
      "\t(No symbol) [0x00007FF7FCF37CDB]\n",
      "\t(No symbol) [0x00007FF7FCF2CF04]\n",
      "\t(No symbol) [0x00007FF7FCF2D089]\n",
      "\t(No symbol) [0x00007FF7FCF2AE09]\n",
      "\t(No symbol) [0x00007FF7FCF2E8FF]\n",
      "\t(No symbol) [0x00007FF7FCFBCBB0]\n",
      "\t(No symbol) [0x00007FF7FCF9BA7A]\n",
      "\t(No symbol) [0x00007FF7FCFBB8CC]\n",
      "\t(No symbol) [0x00007FF7FCF9B823]\n",
      "\t(No symbol) [0x00007FF7FCF675E8]\n",
      "\t(No symbol) [0x00007FF7FCF68751]\n",
      "\tGetHandleVerifier [0x00007FF7FD4147BD+3129501]\n",
      "\tGetHandleVerifier [0x00007FF7FD464D00+3458528]\n",
      "\tGetHandleVerifier [0x00007FF7FD45B05D+3418429]\n",
      "\tGetHandleVerifier [0x00007FF7FD1E687B+844123]\n",
      "\t(No symbol) [0x00007FF7FD090AFF]\n",
      "\t(No symbol) [0x00007FF7FD08C6D4]\n",
      "\t(No symbol) [0x00007FF7FD08C86D]\n",
      "\t(No symbol) [0x00007FF7FD07BD79]\n",
      "\tBaseThreadInitThunk [0x00007FFECCF2259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFECE6AAF38+40]\n",
      "\n",
      "Error extracting sidebar links from https://ai.umbc.edu/: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=130.0.6723.117); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7FD3EE1F5+2972373]\n",
      "\t(No symbol) [0x00007FF7FD0855F0]\n",
      "\t(No symbol) [0x00007FF7FCF257FA]\n",
      "\t(No symbol) [0x00007FF7FCF37CDB]\n",
      "\t(No symbol) [0x00007FF7FCF2CF04]\n",
      "\t(No symbol) [0x00007FF7FCF2D089]\n",
      "\t(No symbol) [0x00007FF7FCF2AE09]\n",
      "\t(No symbol) [0x00007FF7FCF2E8FF]\n",
      "\t(No symbol) [0x00007FF7FCFBCBB0]\n",
      "\t(No symbol) [0x00007FF7FCF9BA7A]\n",
      "\t(No symbol) [0x00007FF7FCFBB8CC]\n",
      "\t(No symbol) [0x00007FF7FCF9B823]\n",
      "\t(No symbol) [0x00007FF7FCF675E8]\n",
      "\t(No symbol) [0x00007FF7FCF68751]\n",
      "\tGetHandleVerifier [0x00007FF7FD4147BD+3129501]\n",
      "\tGetHandleVerifier [0x00007FF7FD464D00+3458528]\n",
      "\tGetHandleVerifier [0x00007FF7FD45B05D+3418429]\n",
      "\tGetHandleVerifier [0x00007FF7FD1E687B+844123]\n",
      "\t(No symbol) [0x00007FF7FD090AFF]\n",
      "\t(No symbol) [0x00007FF7FD08C6D4]\n",
      "\t(No symbol) [0x00007FF7FD08C86D]\n",
      "\t(No symbol) [0x00007FF7FD07BD79]\n",
      "\tBaseThreadInitThunk [0x00007FFECCF2259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFECE6AAF38+40]\n",
      "\n",
      "Error extracting sidebar links from https://cybersecurity.umbc.edu/training/labs/: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=130.0.6723.117); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7FD3EE1F5+2972373]\n",
      "\t(No symbol) [0x00007FF7FD0855F0]\n",
      "\t(No symbol) [0x00007FF7FCF257FA]\n",
      "\t(No symbol) [0x00007FF7FCF37CDB]\n",
      "\t(No symbol) [0x00007FF7FCF2CF04]\n",
      "\t(No symbol) [0x00007FF7FCF2D089]\n",
      "\t(No symbol) [0x00007FF7FCF2AE09]\n",
      "\t(No symbol) [0x00007FF7FCF2E8FF]\n",
      "\t(No symbol) [0x00007FF7FCFBCBB0]\n",
      "\t(No symbol) [0x00007FF7FCF9BA7A]\n",
      "\t(No symbol) [0x00007FF7FCFBB8CC]\n",
      "\t(No symbol) [0x00007FF7FCF9B823]\n",
      "\t(No symbol) [0x00007FF7FCF675E8]\n",
      "\t(No symbol) [0x00007FF7FCF68751]\n",
      "\tGetHandleVerifier [0x00007FF7FD4147BD+3129501]\n",
      "\tGetHandleVerifier [0x00007FF7FD464D00+3458528]\n",
      "\tGetHandleVerifier [0x00007FF7FD45B05D+3418429]\n",
      "\tGetHandleVerifier [0x00007FF7FD1E687B+844123]\n",
      "\t(No symbol) [0x00007FF7FD090AFF]\n",
      "\t(No symbol) [0x00007FF7FD08C6D4]\n",
      "\t(No symbol) [0x00007FF7FD08C86D]\n",
      "\t(No symbol) [0x00007FF7FD07BD79]\n",
      "\tBaseThreadInitThunk [0x00007FFECCF2259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFECE6AAF38+40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"C:/Users/almas/OneDrive/Desktop/Fall 2024/webscraping/research_scraping_log.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def log_update_date(message):\n",
    "    \"\"\"Log a custom message with the current date.\"\"\"\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    logging.info(f\"{message} - Date: {current_date}\")\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "# chrome_options.add_argument(\"--headless\")  # Uncomment to run in headless mode\n",
    "\n",
    "logging.info(\"Initializing WebDriver\")\n",
    "# service = Service('path/to/chromedriver')  # Replace with the correct path\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "logging.info(\"WebDriver initialized successfully\")\n",
    "\n",
    "# Define research links with proper formatting\n",
    "Research_links = {\n",
    "    \"CSEE Research Areas\": \"https://www.csee.umbc.edu/csee-research-areas/\",\n",
    "    \"Research Centers\": \"https://www.csee.umbc.edu/research-focus-areas-and-centers/\",\n",
    "    \"Research Labs CSEE\": \"https://www.csee.umbc.edu/research/research-labs/\",\n",
    "    \"AI Home\": \"https://ai.umbc.edu/\",\n",
    "    \"Research faculty\": \"https://ai.umbc.edu/ai-faculty/\",\n",
    "    \"Research Labs AI\": \"https://ai.umbc.edu/labs-groups/\",\n",
    "    \"Cyber Security labs\": \"https://cybersecurity.umbc.edu/training/labs/\"\n",
    "}\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "visited_links = set()\n",
    "\n",
    "# Define the main scraping process\n",
    "try:\n",
    "    log_update_date(\"Scraping started\")\n",
    "\n",
    "    for section_name, url in Research_links.items():\n",
    "        log_update_date(f\"Scraping started for section: {section_name}\")\n",
    "        \n",
    "        # Extract content from the main page for other sections\n",
    "        main_content = extract_main_content(url, section_name)\n",
    "        \n",
    "        if main_content:\n",
    "            logging.info(f\"Successfully scraped main content for section: {section_name}\")\n",
    "            \n",
    "            # Step 2: Extract collapsible sections (if any)\n",
    "            extract_collapsible_sections(main_content, url, section_name)\n",
    "            logging.info(f\"Collapsible sections extracted for section: {section_name}\")\n",
    "\n",
    "            # Step 3: Identify and visit internal links within the main content\n",
    "            scrape_internal_links(main_content, section_name)\n",
    "            logging.info(f\"Internal links scraped for section: {section_name}\")\n",
    "\n",
    "            # Step 4: Scrape all links in the sidebar\n",
    "            scrape_sidebar_links(url, section_name)\n",
    "            logging.info(f\"Sidebar links scraped for section: {section_name}\")\n",
    "        else:\n",
    "            logging.warning(f\"No main content found for section: {section_name}\")\n",
    "\n",
    "finally:\n",
    "    logging.info(\"Closing WebDriver\")\n",
    "    driver.quit()\n",
    "    logging.info(\"WebDriver closed successfully\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_csee = pd.DataFrame(results, columns=['Section', 'Link', 'Title', 'Text'])\n",
    "output_file = \"C:/Users/almas/OneDrive/Desktop/Fall 2024/webscraping/csee_scraped_data.csv\"\n",
    "df_csee.to_csv(output_file, index=False)\n",
    "logging.info(f\"Data saved to CSV file: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71ddaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to an CSV file\n",
    "df_csee.to_csv('C:/Users/almas/OneDrive/Desktop/Fall 2024/webscraping/csee_scraped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a1946cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csee.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86b141cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communications and Photonics\\nTopics: Cognitive Radio,Â  Nonlinear and Integrated Optics, Optical Fibers, Wireless Communications and Networking\\nFaculty: Gary Carter, Fow-Sen Choa, Anthony Johnson, Seung-Jun Kim, Curtis Menyuk, Dmitri Perkins, Ergun Simsek, Li Yan, Mohamed Younis\\nComputer Architecture and Hardware Systems\\nTopics: Hardware Security, Low-power IC Design, Multicore Systems, Neuromorphic Computing, VLSI DesignTest\\nFaculty: Riadul Islam, Naghmeh Karimi, Chenchen Liu, Dhananjay Phatak, Ryan Robucci\\nCyber-Physical Systems\\nTopics: IoT, Networking, Network Protocols and Management, Network Security, Secure Communication, Smart Power Grids\\nFaculty: Nilanjan Banerjee, Riadul Islam, Anupam Joshi, Naghmeh Karimi, Seung-Jun Kim, Dong Li, Dmitri Perkins, Ryan Robucci, Deepinder Sidhu, Mohamed Younis, Roberto Yus\\nGraphics and Visualization\\nTopics: Character Animation, Computer Graphics, ScientificData Visualization\\nFaculty: Adam Bargteil, Don Engel, Mark Olano, Rebecca Williams\\nAI, Machine Learning, and Signal Processing\\nTopics: AI, Data Analytics, Knowledge Representation, Machine Learning, Matrix and Tensor Factorizations, Natural Language Processing, Computer Vision, Robotics, Statistical Signal Processing\\nFaculty: Tulay Adali, Justin Brooks, Chein-I Chang, Keke Chen, Sanorita Dey, Don Engel,Â  Frank Ferraro, Tim Finin, Tejas Gokhale, Riadul Islam, Manas Gaur, Konstantinos Kalpakis, Seung-Jun Kim, Dong Li, Lara Martin, Cynthia Matuszek, Charles Nicholas, Tim Oates, Ramana Vinjamuri, Rebecca Williams\\nSecurity and Privacy\\nTopics: Computer Security, Cryptography, Cybersecurity, Malware Analysis, Protocols, Secure Voting\\nFaculty: Nilanjan Banerjee, Keke Chen, Tim Finin, Richard Forno,Â  Anupam Joshi, Naghmeh Karimi, Charles Nicholas, Dhananjay Phatak, Alan Sherman, Deepinder Sidhu, Mohamed Younis, Roberto Yus\\nQuantum ComputingTheory and Algorithms\\nTopics: Computational Complexity Theory, Quantum Computation, Quantum Information, Scientific and Engineering Computation\\nFaculty: Richard Chang, Milt Halem, Sam Lomonaco, Curtis Menyuk, Yaacov Yesha\\n\\nÂ \n"
     ]
    }
   ],
   "source": [
    "# Set display options to show full content without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "text = df_csee[df_csee['Title'] == 'CSEE Research Areas']['Text']\n",
    "print(text.to_string(index=False))\n",
    "\n",
    "# Reset the display option to default after printing, if desired\n",
    "pd.reset_option('display.max_colwidth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "133738aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\almas\\anaconda3\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.7 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (0.3.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.16 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (0.3.17)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (0.1.142)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_community) (8.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain<0.4.0,>=0.3.7->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain<0.4.0,>=0.3.7->langchain_community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.16->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.16->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.16->langchain_community) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain_community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.16->langchain_community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain_community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain_community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3425959-2e7d-4f08-b0a5-1f3e29a5feb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "# from langchain.vectorstores.cassandra import Cassandra\n",
    "# import cassio\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80b218cd-b81a-425e-8cf8-e85b27d70545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0265831-73e0-4f6d-9dc2-98c80fe5ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3a5c687-dfe7-4cfe-895d-811b6d58e386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_isss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14570b75-a0bf-4758-b3e2-09dd745d06ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csee.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d08a8349-fb88-426b-9b01-3dfff4f57658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 4)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dil.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c39b9217-70ec-4fb4-8b48-a9f852ef582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Section                                               Link  \\\n",
      "0    CSEE Research Areas     https://www.csee.umbc.edu/csee-research-areas/   \n",
      "1    CSEE Research Areas                https://www.csee.umbc.edu/research/   \n",
      "2       Research Centers  https://www.csee.umbc.edu/research-focus-areas...   \n",
      "3       Research Centers  https://www.csee.umbc.edu/research/research-labs/   \n",
      "4                AI Home                               https://ai.umbc.edu/   \n",
      "..                   ...                                                ...   \n",
      "144              Faculty       https://dil.umbc.edu/faculty/person/yn78817/   \n",
      "145              Faculty       https://dil.umbc.edu/faculty/person/qa97378/   \n",
      "146              Faculty       https://dil.umbc.edu/faculty/person/qo08734/   \n",
      "147              Faculty       https://dil.umbc.edu/faculty/person/bd53800/   \n",
      "148              Faculty                    mailto:datascience-mps@umbc.edu   \n",
      "\n",
      "                                Title  \\\n",
      "0                 CSEE Research Areas   \n",
      "1                            Research   \n",
      "2    Research Focus Areas and Centers   \n",
      "3               Research Laboratories   \n",
      "4                  UMBC Center for AI   \n",
      "..                                ...   \n",
      "144                        John H Wan   \n",
      "145                Chaojie (Jay) Wang   \n",
      "146                           Xin Xue   \n",
      "147                  Waleed A Youssef   \n",
      "148                           Faculty   \n",
      "\n",
      "                                                  Text  \n",
      "0    Communications and Photonics\\nTopics: Cognitiv...  \n",
      "1    Our department has a strong and diverse resear...  \n",
      "2    Focus Areas\\nBrain Discovery\\n\\nThe human brai...  \n",
      "3    Research Centers and Laboratories\\nComputer Sc...  \n",
      "4    The UMBC Center for AI is an interdisciplinary...  \n",
      "..                                                 ...  \n",
      "144   Back to Directory List\\n\\nJohn H Wan\\n\\nInstr...  \n",
      "145   Back to Directory List\\n\\nChaojie (Jay) Wang\\...  \n",
      "146   Back to Directory List\\n\\nXin Xue\\n\\nInstruct...  \n",
      "147   Back to Directory List\\n\\nWaleed A Youssef\\n\\...  \n",
      "148  Full-Time Faculty Members\\nAjinkya Shishir Bor...  \n",
      "\n",
      "[149 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df_csee = pd.DataFrame(results, columns=['Section', 'Link', 'Title', 'Text'])\n",
    "# df_isss = pd.DataFrame(results, columns=['Section', 'Link', 'Title', 'Text'])\n",
    "# df_dil = pd.DataFrame(results, columns=['Section', 'Link', 'Title', 'Text'])\n",
    "\n",
    "# Combine all three DataFrames into one\n",
    "df_combined = pd.concat([df_csee, df_isss, df_dil], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50b0cabf-34d0-4d17-b4ee-3bbbe6fa863b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Faculty</td>\n",
       "      <td>https://dil.umbc.edu/faculty/person/yn78817/</td>\n",
       "      <td>John H Wan</td>\n",
       "      <td>Back to Directory List\\n\\nJohn H Wan\\n\\nInstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Faculty</td>\n",
       "      <td>https://dil.umbc.edu/faculty/person/qa97378/</td>\n",
       "      <td>Chaojie (Jay) Wang</td>\n",
       "      <td>Back to Directory List\\n\\nChaojie (Jay) Wang\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Faculty</td>\n",
       "      <td>https://dil.umbc.edu/faculty/person/qo08734/</td>\n",
       "      <td>Xin Xue</td>\n",
       "      <td>Back to Directory List\\n\\nXin Xue\\n\\nInstruct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Faculty</td>\n",
       "      <td>https://dil.umbc.edu/faculty/person/bd53800/</td>\n",
       "      <td>Waleed A Youssef</td>\n",
       "      <td>Back to Directory List\\n\\nWaleed A Youssef\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Faculty</td>\n",
       "      <td>mailto:datascience-mps@umbc.edu</td>\n",
       "      <td>Faculty</td>\n",
       "      <td>Full-Time Faculty Members\\nAjinkya Shishir Bor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Section                                          Link  \\\n",
       "144  Faculty  https://dil.umbc.edu/faculty/person/yn78817/   \n",
       "145  Faculty  https://dil.umbc.edu/faculty/person/qa97378/   \n",
       "146  Faculty  https://dil.umbc.edu/faculty/person/qo08734/   \n",
       "147  Faculty  https://dil.umbc.edu/faculty/person/bd53800/   \n",
       "148  Faculty               mailto:datascience-mps@umbc.edu   \n",
       "\n",
       "                  Title                                               Text  \n",
       "144          John H Wan   Back to Directory List\\n\\nJohn H Wan\\n\\nInstr...  \n",
       "145  Chaojie (Jay) Wang   Back to Directory List\\n\\nChaojie (Jay) Wang\\...  \n",
       "146             Xin Xue   Back to Directory List\\n\\nXin Xue\\n\\nInstruct...  \n",
       "147    Waleed A Youssef   Back to Directory List\\n\\nWaleed A Youssef\\n\\...  \n",
       "148             Faculty  Full-Time Faculty Members\\nAjinkya Shishir Bor...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "113f8455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_groq in c:\\users\\almas\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_groq) (0.11.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_groq) (0.3.17)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\almas\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (0.1.142)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (8.2.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (3.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84aabd71-c764-4524-afe8-14bd4551a8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\almas\\anaconda3\\lib\\site-packages (1.54.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\almas\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9047c99d-cc92-44b3-afb9-6abb12f63f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\almas\\anaconda3\\lib\\site-packages (0.5.18)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (2.8.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (0.115.5)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (4.11.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (1.28.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (0.13.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (3.10.11)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\almas\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\almas\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.41.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\almas\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\almas\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\almas\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\almas\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\almas\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\almas\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.26.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\almas\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa2d9508-6c76-4090-ab04-03c52fdb015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_openai in c:\\users\\almas\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_openai) (0.3.17)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.54.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_openai) (1.54.4)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain_openai) (0.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.17->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.17->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.17->langchain_openai) (0.1.142)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.17->langchain_openai) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.17->langchain_openai) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.17->langchain_openai) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.17->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (0.7.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (4.66.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain_openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain_openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain_openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain_openai) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain_openai) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.17->langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.17->langchain_openai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\almas\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.54.0->langchain_openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc412e43-8568-423a-bd71-d65bf7ada47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c06f0ee-53f3-4c46-9149-85f397a61235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Section                                            Link  \\\n",
      "0  CSEE Research Areas  https://www.csee.umbc.edu/csee-research-areas/   \n",
      "1  CSEE Research Areas  https://www.csee.umbc.edu/csee-research-areas/   \n",
      "2  CSEE Research Areas  https://www.csee.umbc.edu/csee-research-areas/   \n",
      "3  CSEE Research Areas             https://www.csee.umbc.edu/research/   \n",
      "4  CSEE Research Areas             https://www.csee.umbc.edu/research/   \n",
      "\n",
      "                 Title                                              Chunk  \n",
      "0  CSEE Research Areas  Communications and Photonics\\nTopics: Cognitiv...  \n",
      "1  CSEE Research Areas  Graphics and Visualization\\nTopics: Character ...  \n",
      "2  CSEE Research Areas  Security and Privacy\\nTopics: Computer Securit...  \n",
      "3             Research  Our department has a strong and diverse resear...  \n",
      "4             Research  Find UMBC Faculty Experts\\n\\nAre you looking f...  \n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import pandas as pd\n",
    "\n",
    "# # Example DataFrame (replace with your df_combined)\n",
    "# df_combined = pd.DataFrame([\n",
    "#     {\"Section\": \"CSEE\", \"Link\": \"http://example.com/1\", \"Title\": \"Example 1\", \"Text\": \"This is a long text that needs splitting.\"},\n",
    "#     {\"Section\": \"ISSS\", \"Link\": \"http://example.com/2\", \"Title\": \"Example 2\", \"Text\": \"Another long text example for testing splitting functionality.\"}\n",
    "# ])\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "\n",
    "# List to store split documents\n",
    "split_docs = []\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for _, row in df_combined.iterrows():\n",
    "    # Create a Document object\n",
    "    text_document = Document(page_content=row[\"Text\"], metadata={\n",
    "        \"Section\": row[\"Section\"],\n",
    "        \"Link\": row[\"Link\"],\n",
    "        \"Title\": row[\"Title\"]\n",
    "    })\n",
    "    \n",
    "    # Split the Document into chunks\n",
    "    docs = text_splitter.split_documents([text_document])\n",
    "    \n",
    "    # Append each chunk along with metadata to the result\n",
    "    for doc in docs:\n",
    "        split_docs.append({\n",
    "            \"Section\": doc.metadata[\"Section\"],\n",
    "            \"Link\": doc.metadata[\"Link\"],\n",
    "            \"Title\": doc.metadata[\"Title\"],\n",
    "            \"Chunk\": doc.page_content\n",
    "        })\n",
    "\n",
    "# Create a new DataFrame with split chunks\n",
    "df_split = pd.DataFrame(split_docs)\n",
    "\n",
    "# Display the first few rows of the split DataFrame\n",
    "print(df_split.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9d72b-d889-4760-a0c0-c24e34e66bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored successfully in Chroma DB!\n"
     ]
    }
   ],
   "source": [
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Set your OpenAI API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Replace with your actual OpenAI API key\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Convert the split DataFrame into Chroma-compatible format\n",
    "documents = [\n",
    "    Document(page_content=row[\"Chunk\"], metadata={\n",
    "        \"Section\": row[\"Section\"],\n",
    "        \"Link\": row[\"Link\"],\n",
    "        \"Title\": row[\"Title\"]\n",
    "    })\n",
    "    for _, row in df_split.iterrows()\n",
    "]\n",
    "\n",
    "# Store embeddings in Chroma DB\n",
    "db = Chroma.from_documents(documents, embeddings, persist_directory=\"chroma_data\")\n",
    "\n",
    "# Persist the database for reuse across sessions\n",
    "db.persist()\n",
    "\n",
    "print(\"Embeddings stored successfully in Chroma DB!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54e73244-df6e-486a-afc5-5aee92cca33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "# Wrap the vector store for querying\n",
    "chroma_index = VectorStoreIndexWrapper(vectorstore=db)\n",
    "\n",
    "# # Example query\n",
    "# query = \"What pathways are available in the UMBC Data Science program?\"\n",
    "# results = chroma_index.query(query)\n",
    "\n",
    "# print(\"\\nQuery Results:\")\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "53db1640-f17d-4b14-8362-c4de630f818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=os.environ['groq_api_key']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa2e2d2e-e883-4d93-9be1-f5a021f8bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(groq_api_key=groq_api_key,\n",
    "         model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c939b315-5e21-4942-9f27-1a72f0c1863c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the following are the different pathways available for students:\\n\\n1. Cybersecurity Pathway\\n2. Economics/Econometrics (Only available at the main campus)\\n3. Healthcare Analytics (Only available at the main campus)\\n4. Policy Analysis (Only available at the main campus)\\n5. Project Management\\n6. Management Sciences\\n7. Aging Studies (Online)\\n8. Advanced Computing and Analytics Pathway (Only available at the main campus)\\n9. Bioinformatics (FAES NIH)\\n10. Clinical Informatics (with UMB)\\n\\nThese pathways are designed to allow students who work in a particular domain to take classes specific to their industry.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_index.query(\"What are the different pathways available list out all\",llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b8850414-202e-4b0d-bcaa-3d8f3e1ca0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the information provided, you should begin the OPT application process by requesting the OPT I-20 from the ISSS office early. Once you have the OPT I-20, you can then prepare and submit the OPT application to USCIS. It's important to note that OPT applications must be submitted to USCIS no earlier than 90 days before your program end date and no later than 60 days after your program end date. Therefore, you should aim to have your OPT I-20 ready and application prepared within the 90-day window before your program end date. Working with the ISSS office early in the process will help ensure that you meet these deadlines.\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_index.query(\"How early should i begin my opt application process\",llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88fb381b-89df-4ba9-b00a-6e6dda4b1160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To apply for the program, you should follow these steps:\\n\\n1. Go to the application website.\\n2. Submit your online application.\\n3. Include the following documents in your application:\\n\\t* A statement outlining your goals and expectations in the program, as well as your background and qualifications.\\n\\t* Your current resume.\\n\\t* Transcripts from each college or university you have attended.\\n\\t* Two reference letters.\\n\\t* If you are an international student, a TOEFL, IELTS, or PTE test score.\\n\\t* A non-refundable application fee of $50.\\n\\t* If you are a local student, a completed application including a residency form to determine in-state tuition eligibility.\\n\\nIncomplete applications will not be reviewed. If you have already started an application, you can finish it up.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_index.query(\"How do i apply for GA\",llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "09a2ae25-f2da-4bbf-ac42-28c734f5e12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, I can tell you that DATA 603 is a course that requires some familiarity with big data platforms. If you have such familiarity, you can take it in your first semester, otherwise, it is recommended to take it after completing DATA 601. If you are taking one or two courses per semester, you might want to consider taking DATA 603 after DATA 602. However, the syllabus for DATA 603 is not provided in the information given. I would recommend checking the course catalog or contacting the department directly for detailed course information.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_index.query(\"What is the syllabus of 603\",llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19cbdfed-bffe-4627-a469-830f0c7aa2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, if you are familiar with data analysis and machine learning or have an undergraduate degree in Computer Science or Information Systems, you can take DATA 602 concurrently with DATA 601 in your first semester. However, if you are not familiar with these topics, it is recommended to take DATA 602 after completing DATA 601. Additionally, if you are not familiar with big data platforms, it is recommended to take DATA 603 after completing DATA 601. Therefore, taking DATA 602 in your first semester is possible under certain conditions.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_index.query(\"Can I take 602 in my first semester\",llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a470a5d4-7b53-4fd5-8bb8-15b2cbcc7246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To apply for a GGT (graduate graduate stutor) position in the MPS Data Science program, you need to complete Data 601, 602, 603, and 604 and formally apply through the specified page. There is no need to get a recommendation from a faculty member for your application to be considered. Sending personal emails to faculty members, including the directors, will not increase your chances of receiving a GGT position.\\n\\nYou can find the link to apply on the page that contains the information you provided. Since I cannot share the link directly, I recommend searching for \"MPS Data Science GGT application\" or a similar query in your web browser. Make sure you are on the official program website before proceeding with your application.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_index.query(\"How do i apply for ggt\",llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b85b4cf6-c329-41a9-b28b-adce36ee4777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "{'input': 'How do i apply for GA?', 'context': [Document(metadata={'Link': 'https://dil.umbc.edu/prospective-students/master-of-professional-studies-in-data-science/', 'Section': 'Prospective Students', 'Title': 'Domestic Applicants'}, page_content='Your application should include the following documents: (Incomplete applications will not be reviewed. If you already have started an application, you can finish up)\\n\\nStatement outlining your goals and expectations in the program. This also is an opportunity for you to further explain your background and qualifications for the program and provide any additional information about you or your experience which will help with the evaluation of your application.\\nCurrent resume (When applying online, upload your resume to the graduate school application.)\\nTranscript(s) from each college  university attended\\nLocal Students: Completed application including residency form (Residency form determines in-state tuition eligibility)\\nInternational Students: TOEFL, IELTS or PTE test score\\nNon-refundable application fee (50 online)\\n\\nHow do I apply?\\n\\nBy submitting an online application. Herere the specific steps:'), Document(metadata={'Link': 'https://dil.umbc.edu/prospective-students/master-of-professional-studies-in-data-science/', 'Section': 'Prospective Students', 'Title': 'Domestic Applicants'}, page_content='Your application should include the following documents: (Incomplete applications will not be reviewed. If you already have started an application, you can finish up)\\n\\nStatement outlining your goals and expectations in the program. This also is an opportunity for you to further explain your background and qualifications for the program and provide any additional information about you or your experience which will help with the evaluation of your application.\\nCurrent resume (When applying online, upload your resume to the graduate school application.)\\nTranscript(s) from each college  university attended\\nLocal Students: Completed application including residency form (Residency form determines in-state tuition eligibility)\\nInternational Students: TOEFL, IELTS or PTE test score\\nNon-refundable application fee (50 online)\\n\\nHow do I apply?\\n\\nBy submitting an online application. Herere the specific steps:'), Document(metadata={'Link': 'https://dil.umbc.edu/prospective-students/master-of-professional-studies-in-data-science/', 'Section': 'Prospective Students', 'Title': 'Domestic Applicants'}, page_content='Your application should include the following documents: (Incomplete applications will not be reviewed. If you already have started an application, you can finish up)\\n\\nStatement outlining your goals and expectations in the program. This also is an opportunity for you to further explain your background and qualifications for the program and provide any additional information about you or your experience which will help with the evaluation of your application.\\nCurrent resume (When applying online, upload your resume to the graduate school application.)\\nTranscript(s) from each college  university attended\\nLocal Students: Completed application including residency form (Residency form determines in-state tuition eligibility)\\nInternational Students: TOEFL, IELTS or PTE test score\\nNon-refundable application fee (50 online)\\n\\nHow do I apply?\\n\\nBy submitting an online application. Herere the specific steps:'), Document(metadata={'Link': 'https://dil.umbc.edu/prospective-students/international-students/', 'Section': 'Prospective Students', 'Title': 'International Applicants'}, page_content='Your application should include the following documents: (Incomplete applications will not be reviewed. If you already have started an application, you can finish up)\\n\\nStatement outlining your goals and expectations in the program. This also is an opportunity for you to further explain your background and qualifications for the program and provide any additional information about you or your experience which will help with the evaluation of your application.\\nCurrent resume (When applying online, upload your resume to the graduate school application.)\\nTranscript(s) from each college  university attended\\n2 reference letters\\nInternational Students: TOEFL, IELTS, or PTE test score\\nNon-refundable application fee (50 online)\\n\\nHow do I apply?\\n\\nBy submitting an online application. Herere the specific steps:\\n\\nGo to the application website')], 'answer': 'To apply for the program, you should follow these steps:\\n\\n1. Go to the application website.\\n2. Prepare the necessary documents for your application:\\n   - A statement outlining your goals and expectations in the program, as well as your background and qualifications.\\n   - Your current resume.\\n   - Transcripts from each college or university you have attended.\\n   - If you are a local student, a completed application including a residency form to determine in-state tuition eligibility.\\n   - If you are an international student, TOEFL, IELTS, or PTE test score.\\n   - Two reference letters.\\n   - A non-refundable application fee of $50.\\n3. Upload the required documents and submit your online application.\\n\\nPlease note that incomplete applications will not be reviewed. If you have already started an application, you can finish it up by logging back into the application website and providing any remaining information or documents.'}\n"
     ]
    }
   ],
   "source": [
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# retriever=astra_vector_store.as_retriever()\n",
    "# document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "# retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Assuming db is your Chroma instance\n",
    "retriever = db.as_retriever()  # Use Chroma's retriever\n",
    "\n",
    "# Create a documents chain with LLM and prompt\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Combine the retriever and document chain into a retrieval chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Example query\n",
    "query = \"How do i apply for GA?\"\n",
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "\n",
    "# # Example query\n",
    "# query = \"What pathways are available in the UMBC Data Science program?\"\n",
    "\n",
    "# # Call the chain\n",
    "# response = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "# print(\"Response:\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2681338b-8309-40b6-ac76-0ccedb5493b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What pathways are available in the UMBC Data Science program?\n",
      "Response: {'input': 'What pathways are available in the UMBC Data Science program?', 'context': [Document(metadata={'Link': 'https://dil.umbc.edu/pathways-and-certificates/list-of-available-pathways-pathway-courses/', 'Section': 'Pathways & Certificates', 'Title': 'List of Available Pathways  Elective Courses'}, page_content='The pathways will allow students who work in a particular domain to take classes specific to their industry. Students are recommended to choose a pathway and take three courses from the same pathway. However, in case of class unavailability, students are welcome to choose courses from different pathways. We offer a bunch of DATA 690 Special Topics classes. Students, who meet the prerequisites of these Special Classes, can take up to 5 Special Topics classes.\\n\\nElective Data Science Courses\\nCybersecurity Pathway\\nEconomicsEconometrics (Only at the main campus)\\nHealthcare Analytics (Only at the main campus)\\nPolicy Analysis (Only at the main campus)\\nProject Management\\nManagement Sciences\\nAging Studies (Online)\\nAdvanced Computing and Analytics Pathway (Only at the main campus)\\nBioinformatics (FAES  NIH)\\nClinical Informatics (with UMB)\\n\\n\\xa0'), Document(metadata={'Link': 'https://dil.umbc.edu/pathways-and-certificates/list-of-available-pathways-pathway-courses/', 'Section': 'Pathways & Certificates', 'Title': 'List of Available Pathways  Elective Courses'}, page_content='The pathways will allow students who work in a particular domain to take classes specific to their industry. Students are recommended to choose a pathway and take three courses from the same pathway. However, in case of class unavailability, students are welcome to choose courses from different pathways. We offer a bunch of DATA 690 Special Topics classes. Students, who meet the prerequisites of these Special Classes, can take up to 5 Special Topics classes.\\n\\nElective Data Science Courses\\nCybersecurity Pathway\\nEconomicsEconometrics (Only at the main campus)\\nHealthcare Analytics (Only at the main campus)\\nPolicy Analysis (Only at the main campus)\\nProject Management\\nManagement Sciences\\nAging Studies (Online)\\nAdvanced Computing and Analytics Pathway (Only at the main campus)\\nBioinformatics (FAES  NIH)\\nClinical Informatics (with UMB)\\n\\n\\xa0'), Document(metadata={'Link': 'https://dil.umbc.edu/pathways-and-certificates/list-of-available-pathways-pathway-courses/', 'Section': 'Pathways & Certificates', 'Title': 'List of Available Pathways  Elective Courses'}, page_content='The pathways will allow students who work in a particular domain to take classes specific to their industry. Students are recommended to choose a pathway and take three courses from the same pathway. However, in case of class unavailability, students are welcome to choose courses from different pathways. We offer a bunch of DATA 690 Special Topics classes. Students, who meet the prerequisites of these Special Classes, can take up to 5 Special Topics classes.\\n\\nElective Data Science Courses\\nCybersecurity Pathway\\nEconomicsEconometrics (Only at the main campus)\\nHealthcare Analytics (Only at the main campus)\\nPolicy Analysis (Only at the main campus)\\nProject Management\\nManagement Sciences\\nAging Studies (Online)\\nAdvanced Computing and Analytics Pathway (Only at the main campus)\\nBioinformatics (FAES  NIH)\\nClinical Informatics (with UMB)\\n\\n\\xa0'), Document(metadata={'Link': 'https://dil.umbc.edu/pathways-and-certificates/list-of-available-pathways-pathway-courses/', 'Section': 'Pathways & Certificates', 'Title': 'List of Available Pathways  Elective Courses'}, page_content='The pathways will allow students who work in a particular domain to take classes specific to their industry. Students are recommended to choose a pathway and take three courses from the same pathway. However, in case of class unavailability, students are welcome to choose courses from different pathways. We offer a bunch of DATA 690 Special Topics classes. Students, who meet the prerequisites of these Special Classes, can take up to 5 Special Topics classes.\\n\\nElective Data Science Courses\\nCybersecurity Pathway\\nEconomicsEconometrics (Only at the main campus)\\nHealthcare Analytics (Only at the main campus)\\nPolicy Analysis (Only at the main campus)\\nProject Management\\nManagement Sciences\\nAging Studies (Online)\\nAdvanced Computing and Analytics Pathway (Only at the main campus)\\nBioinformatics (FAES  NIH)\\nClinical Informatics (with UMB)\\n\\n\\xa0')], 'answer': 'Based on the provided context, the available pathways in the UMBC Data Science program are:\\n\\n1. Cybersecurity Pathway\\n2. Economics and Econometrics (Only at the main campus)\\n3. Healthcare Analytics (Only at the main campus)\\n4. Policy Analysis (Only at the main campus)\\n5. Project Management\\n6. Management Sciences\\n7. Aging Studies (Online)\\n8. Advanced Computing and Analytics Pathway (Only at the main campus)\\n9. Bioinformatics (FAES NIH)\\n10. Clinical Informatics (with UMB)\\n\\nPlease note that some pathways have specific restrictions, such as being available only at the main campus or online.'}\n",
      "\n",
      "Query: Can i take 602 in my first semester?\n",
      "Response: {'input': 'Can i take 602 in my first semester?', 'context': [Document(metadata={'Link': 'https://dil.umbc.edu/courses/what-classes-should-i-take-in-the-upcoming-semester/', 'Section': 'Courses', 'Title': 'What classes should I take in the upcoming semester?'}, page_content='If you are familiar with data analysis and machine learning or if you have an undergraduate degree in Computer Science or Information Systems, you can take DATA 602 concurrently with DATA 601 but if it is not the case, then please take 602 after taking 601.\\nIf you already are familiar with big data platforms, you can take DATA 603 even in your first semester. Otherwise, please take it after completing DATA 601.\\nIf you are taking one or two courses per semester, then please consider taking DATA 603 after taking DATA 602.\\nAdditionally, you can only take DATA 606 (Capstone Project in Data Science) after completing your other required DATA courses  most students take that course in their final year but are thinking about potential research topics long before then.\\nBeyond those simple guidelines, below is some additional advising information for you to consider  save for future reference during your time with us.'), Document(metadata={'Link': 'https://dil.umbc.edu/courses/what-classes-should-i-take-in-the-upcoming-semester/', 'Section': 'Courses', 'Title': 'What classes should I take in the upcoming semester?'}, page_content='If you are familiar with data analysis and machine learning or if you have an undergraduate degree in Computer Science or Information Systems, you can take DATA 602 concurrently with DATA 601 but if it is not the case, then please take 602 after taking 601.\\nIf you already are familiar with big data platforms, you can take DATA 603 even in your first semester. Otherwise, please take it after completing DATA 601.\\nIf you are taking one or two courses per semester, then please consider taking DATA 603 after taking DATA 602.\\nAdditionally, you can only take DATA 606 (Capstone Project in Data Science) after completing your other required DATA courses  most students take that course in their final year but are thinking about potential research topics long before then.\\nBeyond those simple guidelines, below is some additional advising information for you to consider  save for future reference during your time with us.'), Document(metadata={'Link': 'https://dil.umbc.edu/courses/what-classes-should-i-take-in-the-upcoming-semester/', 'Section': 'Courses', 'Title': 'What classes should I take in the upcoming semester?'}, page_content='If you are familiar with data analysis and machine learning or if you have an undergraduate degree in Computer Science or Information Systems, you can take DATA 602 concurrently with DATA 601 but if it is not the case, then please take 602 after taking 601.\\nIf you already are familiar with big data platforms, you can take DATA 603 even in your first semester. Otherwise, please take it after completing DATA 601.\\nIf you are taking one or two courses per semester, then please consider taking DATA 603 after taking DATA 602.\\nAdditionally, you can only take DATA 606 (Capstone Project in Data Science) after completing your other required DATA courses  most students take that course in their final year but are thinking about potential research topics long before then.\\nBeyond those simple guidelines, below is some additional advising information for you to consider  save for future reference during your time with us.'), Document(metadata={'Link': 'https://dil.umbc.edu/courses/what-classes-should-i-take-in-the-upcoming-semester/', 'Section': 'Courses', 'Title': 'What classes should I take in the upcoming semester?'}, page_content='Note that all the DATA 690 courses are counted towards your degree.\\nWHEN SHOULD I TAKE DATA 602, DATA 603, and DATA 606?\\nIf you are familiar with data analysis and machine learning or if you have an undergraduate degree in Computer Science or Information Systems, you can take DATA 602 concurrently with DATA 601 but if it is not the case, then please take 602 after taking 601.\\nIf you already are familiar with big data platforms, you can take DATA 603 even in your first semester. Otherwise, please take it after completing DATA 601.\\nIf you are taking one or two courses per semester, then please consider taking DATA 603 after taking DATA 602.\\nAdditionally, you can only take DATA 606 (Capstone Project in Data Science) after completing your other required DATA courses  most students take that course in their final year but are thinking about potential research topics long before then.')], 'answer': 'Based on the provided context, whether you can take DATA 602 in your first semester depends on your familiarity with data analysis and machine learning or your undergraduate degree in Computer Science or Information Systems.\\n\\nIf you have this background, you can take DATA 602 concurrently with DATA 601 in your first semester. However, if you do not have this background, it is recommended to take DATA 602 after completing DATA 601.\\n\\nTherefore, it is possible to take DATA 602 in your first semester if you meet the required criteria, but if not, it is recommended to take it in a later semester.'}\n",
      "\n",
      "Query: What electives can I take for Cybersecurity?\n",
      "Response: {'input': 'What electives can I take for Cybersecurity?', 'context': [Document(metadata={'Link': 'https://dil.umbc.edu/pathways-and-certificates/list-of-available-pathways-pathway-courses/', 'Section': 'Pathways & Certificates', 'Title': 'Cybersecurity Pathway'}, page_content='The Cybersecurity pathway provides broad exposure to cybersecurity principles, best practices, and relevant technologies. Its purpose is to offer students from varying technical disciplines foundational knowledge and the preparation to effectively incorporate cybersecurity concepts, thinking, and analysis into their own professional activities.\\nThis pathway leads to a Graduate Certificate in Cybersecurity Operations. Students who choose this pathway need to contact the CYBER GPD to learn when these courses will be offered in the following semester(s).\\nStudents must take three required cybersecurity courses and one data science elective (DATA 601, 602, or 605). The CYBER courses in order to complete the pathway and get the certificate are\\nCYBR 620 Introduction to Cybersecurity\\nCYBR 650 Managing Cybersecurity Operations\\nCYBR 658 Risk Analysis and Compliance'), Document(metadata={'Link': 'https://dil.umbc.edu/pathways-and-certificates/list-of-available-pathways-pathway-courses/', 'Section': 'Pathways & Certificates', 'Title': 'Cybersecurity Pathway'}, page_content='The Cybersecurity pathway provides broad exposure to cybersecurity principles, best practices, and relevant technologies. Its purpose is to offer students from varying technical disciplines foundational knowledge and the preparation to effectively incorporate cybersecurity concepts, thinking, and analysis into their own professional activities.\\nThis pathway leads to a Graduate Certificate in Cybersecurity Operations. Students who choose this pathway need to contact the CYBER GPD to learn when these courses will be offered in the following semester(s).\\nStudents must take three required cybersecurity courses and one data science elective (DATA 601, 602, or 605). The CYBER courses in order to complete the pathway and get the certificate are\\nCYBR 620 Introduction to Cybersecurity\\nCYBR 650 Managing Cybersecurity Operations\\nCYBR 658 Risk Analysis and Compliance'), Document(metadata={'Link': 'https://dil.umbc.edu/pathways-and-certificates/list-of-available-pathways-pathway-courses/', 'Section': 'Pathways & Certificates', 'Title': 'Cybersecurity Pathway'}, page_content='The Cybersecurity pathway provides broad exposure to cybersecurity principles, best practices, and relevant technologies. Its purpose is to offer students from varying technical disciplines foundational knowledge and the preparation to effectively incorporate cybersecurity concepts, thinking, and analysis into their own professional activities.\\nThis pathway leads to a Graduate Certificate in Cybersecurity Operations. Students who choose this pathway need to contact the CYBER GPD to learn when these courses will be offered in the following semester(s).\\nStudents must take three required cybersecurity courses and one data science elective (DATA 601, 602, or 605). The CYBER courses in order to complete the pathway and get the certificate are\\nCYBR 620 Introduction to Cybersecurity\\nCYBR 650 Managing Cybersecurity Operations\\nCYBR 658 Risk Analysis and Compliance'), Document(metadata={'Link': 'https://dil.umbc.edu/pathways-and-certificates/list-of-available-pathways-pathway-courses/', 'Section': 'Pathways & Certificates', 'Title': 'Cybersecurity Pathway'}, page_content='The Cybersecurity pathway provides broad exposure to cybersecurity principles, best practices, and relevant technologies. Its purpose is to offer students from varying technical disciplines foundational knowledge and the preparation to effectively incorporate cybersecurity concepts, thinking, and analysis into their own professional activities.\\nThis pathway leads to a Graduate Certificate in Cybersecurity Operations. Students who choose this pathway need to contact the CYBER GPD to learn when these courses will be offered in the following semester(s).\\nStudents must take three required cybersecurity courses and one data science elective (DATA 601, 602, or 605). The CYBER courses in order to complete the pathway and get the certificate are\\nCYBR 620 Introduction to Cybersecurity\\nCYBR 650 Managing Cybersecurity Operations\\nCYBR 658 Risk Analysis and Compliance')], 'answer': 'Based on the provided context, if you are a student in the Cybersecurity pathway, you can choose one elective from the following data science courses to complete your Graduate Certificate in Cybersecurity Operations:\\n\\n- DATA 601: Data Visualization\\n- DATA 602: Machine Learning\\n- DATA 605: Big Data Analytics\\n\\nThese electives are listed in the context as the data science courses (DATA 601, 602, or 605) that students can take as part of the Cybersecurity pathway.'}\n",
      "\n",
      "Query: How do I apply for OPT?\n",
      "Response: {'input': 'How do I apply for OPT?', 'context': [Document(metadata={'Link': 'https://isss.umbc.edu/opt-and-opt-stem-information/', 'Section': 'OPT and OPT STEM Information', 'Title': 'OPT and OPT STEM Information - Main Content'}, page_content='Second, students will need to upload a draft I-765. Students should download a draft I-765 by clicking â€œView Draft Snapshotâ€ in their online USCIS account. Students who upload a draft I-765 with the incorrect format will need to re-upload it in the correct format.\\n\\nISSS will review each studentâ€™s OPT request and send an email if anything is missing. Once approved, students will receive an I-20 with the OPT recommendation listed on the second page. The I-20 should list â€œOPT Requestedâ€ on the second page, with requested start and end dates. Please contact us if anything needs to be corrected.\\n\\nTo doâ€™s:\\n\\nWatch a previous OPT Workshop presentation (requires myUMBC login) and review the slides\\nDownload a draft I-765 application\\nSubmit a complete OPT request in the ISSS Portal\\n\\nRemember: you do not need a job offer before applying!\\n\\n\\xa0\\n\\nStep 2: Submit I-765 OPT Application to USCIS'), Document(metadata={'Link': 'https://isss.umbc.edu/opt-and-opt-stem-information/', 'Section': 'OPT and OPT STEM Information', 'Title': 'OPT and OPT STEM Information - Main Content'}, page_content='Second, students will need to upload a draft I-765. Students should download a draft I-765 by clicking â€œView Draft Snapshotâ€ in their online USCIS account. Students who upload a draft I-765 with the incorrect format will need to re-upload it in the correct format.\\n\\nISSS will review each studentâ€™s OPT request and send an email if anything is missing. Once approved, students will receive an I-20 with the OPT recommendation listed on the second page. The I-20 should list â€œOPT Requestedâ€ on the second page, with requested start and end dates. Please contact us if anything needs to be corrected.\\n\\nTo doâ€™s:\\n\\nWatch a previous OPT Workshop presentation (requires myUMBC login) and review the slides\\nDownload a draft I-765 application\\nSubmit a complete OPT request in the ISSS Portal\\n\\nRemember: you do not need a job offer before applying!\\n\\n\\xa0\\n\\nStep 2: Submit I-765 OPT Application to USCIS'), Document(metadata={'Link': 'https://isss.umbc.edu/opt-and-opt-stem-information/', 'Section': 'OPT and OPT STEM Information', 'Title': 'OPT and OPT STEM Information - Main Content'}, page_content='Second, students will need to upload a draft I-765. Students should download a draft I-765 by clicking â€œView Draft Snapshotâ€ in their online USCIS account. Students who upload a draft I-765 with the incorrect format will need to re-upload it in the correct format.\\n\\nISSS will review each studentâ€™s OPT request and send an email if anything is missing. Once approved, students will receive an I-20 with the OPT recommendation listed on the second page. The I-20 should list â€œOPT Requestedâ€ on the second page, with requested start and end dates. Please contact us if anything needs to be corrected.\\n\\nTo doâ€™s:\\n\\nWatch a previous OPT Workshop presentation (requires myUMBC login) and review the slides\\nDownload a draft I-765 application\\nSubmit a complete OPT request in the ISSS Portal\\n\\nRemember: you do not need a job offer before applying!\\n\\n\\xa0\\n\\nStep 2: Submit I-765 OPT Application to USCIS'), Document(metadata={'Link': 'https://isss.umbc.edu/opt-and-opt-stem-information/', 'Section': 'OPT and OPT STEM Information', 'Title': 'OPT and OPT STEM Information - Main Content'}, page_content='Second, students will need to upload a draft I-765. Students should download a draft I-765 by clicking â€œView Draft Snapshotâ€ in their online USCIS account. Students who upload a draft I-765 with the incorrect format will need to re-upload it in the correct format.\\n\\nISSS will review each studentâ€™s OPT request and send an email if anything is missing. Once approved, students will receive an I-20 with the OPT recommendation listed on the second page. The I-20 should list â€œOPT Requestedâ€ on the second page, with requested start and end dates. Please contact us if anything needs to be corrected.\\n\\nTo doâ€™s:\\n\\nWatch a previous OPT Workshop presentation (requires myUMBC login) and review the slides\\nDownload a draft I-765 application\\nSubmit a complete OPT request in the ISSS Portal\\n\\nRemember: you do not need a job offer before applying!\\n\\n\\xa0\\n\\nStep 2: Submit I-765 OPT Application to USCIS')], 'answer': 'To apply for OPT, follow these steps based on the provided context:\\n\\n1. Watch a previous OPT Workshop presentation and review the slides. This will help you understand the process and requirements better. (Note: This step requires a myUMBC login.)\\n\\n2. Download a draft I-765 application. You can obtain this draft by logging into your online USCIS account and clicking on \"View Draft Snapshot.\"\\n\\n3. Submit a complete OPT request in the ISSS Portal. After downloading the draft I-765 application, you need to submit your complete OPT request through the ISSS Portal.\\n\\nRemember that you do not need a job offer before applying for OPT.\\n\\nAdditionally, ISSS will review your OPT request and notify you via email if anything is missing. Once approved, you will receive an I-20 with the OPT recommendation listed on the second page, indicating the requested start and end dates. If there are any corrections needed, contact the ISSS for assistance.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ask_question(query):\n",
    "    response = retrieval_chain.invoke({\"input\": query})\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Ask different questions\n",
    "ask_question(\"What pathways are available in the UMBC Data Science program?\")\n",
    "ask_question(\"Can i take 602 in my first semester?\")\n",
    "ask_question(\"What electives can I take for Cybersecurity?\")\n",
    "ask_question(\"How do I apply for OPT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d5f2db74-f58a-45d6-9474-47ce495b95a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\users\\almas\\anaconda3\\lib\\site-packages (0.115.5)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\almas\\anaconda3\\lib\\site-packages (0.32.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from fastapi) (0.41.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from fastapi) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from fastapi) (4.11.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\almas\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.20.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from starlette<0.42.0,>=0.40.0->fastapi) (4.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastapi uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d055b9a1-6c67-4beb-aa05-b1a73cd4cf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated date: 2024-11-17 22:22:13\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_last_updated_date(log_file_path):\n",
    "    \"\"\"\n",
    "    Extract the last updated date from the log file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Look for the last date in the logs\n",
    "        for line in reversed(lines):\n",
    "            # Assuming log format: \"2024-11-16 11:30:00 - INFO - Scraping completed\"\n",
    "            match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', line)\n",
    "            if match:\n",
    "                return match.group(0)  # Return the last matching timestamp\n",
    "        return \"Unknown\"  # If no date is found\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading log file: {e}\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Example Usage\n",
    "log_file_path = \"C:/Users/almas/OneDrive/Desktop/Fall 2024/webscraping/scraping_log.log\"\n",
    "last_updated_date = get_last_updated_date(log_file_path)\n",
    "print(f\"Last updated date: {last_updated_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ec127bfd-8e87-40bc-b4b5-e52e1d3a576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: FastAPI in c:\\users\\almas\\anaconda3\\lib\\site-packages (0.115.5)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\almas\\anaconda3\\lib\\site-packages (0.32.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from FastAPI) (0.41.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from FastAPI) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from FastAPI) (4.11.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\almas\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->FastAPI) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->FastAPI) (2.20.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from starlette<0.42.0,>=0.40.0->FastAPI) (4.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->FastAPI) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\almas\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->FastAPI) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install FastAPI uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068eadc8-77da-46a5-a604-6967c6db91d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [48416]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming Request: {\n",
      "  \"responseId\": \"1f1680f3-0359-421f-aa25-683593eec036-c2998605\",\n",
      "  \"queryResult\": {\n",
      "    \"queryText\": \"can i take 602 in my first semester\",\n",
      "    \"parameters\": {\n",
      "      \"unit-length\": {\n",
      "        \"amount\": 602.0,\n",
      "        \"unit\": \"inch\"\n",
      "      },\n",
      "      \"ordinal\": 1.0\n",
      "    },\n",
      "    \"allRequiredParamsPresent\": true,\n",
      "    \"fulfillmentMessages\": [\n",
      "      {\n",
      "        \"text\": {\n",
      "          \"text\": [\n",
      "            \"\"\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"outputContexts\": [\n",
      "      {\n",
      "        \"name\": \"projects/retreiverbot-sheg/agent/sessions/2a91254e-7727-9427-3440-bb4ec4a68ba6/contexts/__system_counters__\",\n",
      "        \"parameters\": {\n",
      "          \"no-input\": 0.0,\n",
      "          \"no-match\": 0.0,\n",
      "          \"unit-length\": {\n",
      "            \"amount\": 602.0,\n",
      "            \"unit\": \"inch\"\n",
      "          },\n",
      "          \"unit-length.original\": \"602 in\",\n",
      "          \"ordinal\": 1.0,\n",
      "          \"ordinal.original\": \"first\"\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"intent\": {\n",
      "      \"name\": \"projects/retreiverbot-sheg/agent/intents/3d9f8f75-8c98-46db-aa04-7d6337cba9ab\",\n",
      "      \"displayName\": \"Courses_and_Electives_Information\"\n",
      "    },\n",
      "    \"intentDetectionConfidence\": 1.0,\n",
      "    \"languageCode\": \"en\"\n",
      "  },\n",
      "  \"originalDetectIntentRequest\": {\n",
      "    \"source\": \"DIALOGFLOW_CONSOLE\",\n",
      "    \"payload\": {}\n",
      "  },\n",
      "  \"session\": \"projects/retreiverbot-sheg/agent/sessions/2a91254e-7727-9427-3440-bb4ec4a68ba6\"\n",
      "}\n",
      "Triggered Intent: Courses_and_Electives_Information\n",
      "Query: What electives can I take for Cybersecurity?\n",
      "Response: You can take courses like Advanced Network Security, Secure Coding, and Cloud Security.\n",
      "\n",
      "INFO:     64.233.172.133:0 - \"POST /webhook HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "import json\n",
    "import re\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Mock retrieval_chain for testing\n",
    "class MockRetrievalChain:\n",
    "    def invoke(self, query):\n",
    "        mock_responses = {\n",
    "            \"What electives can I take for Cybersecurity?\": \"You can take courses like Advanced Network Security, Secure Coding, and Cloud Security.\",\n",
    "            \"What resources are available for current students?\": \"Current students have access to academic advising, library resources, and career services.\",\n",
    "            \"Who are the faculty members in the Data Science program?\": \"Faculty members include Dr. Smith, Dr. Johnson, and Dr. Davis.\",\n",
    "            \"What are the rules for OPT and CPT?\": \"OPT allows students to work in the U.S. for up to 12 months post-graduation, while CPT is for internships during the program.\",\n",
    "            \"What should prospective students know about the UMBC Data Science program?\": \"Prospective students should know about program requirements, tuition fees, and career opportunities.\",\n",
    "            \"What research opportunities are available in UMBC Data Science?\": \"Research opportunities include AI, cybersecurity, and health informatics projects.\"\n",
    "        }\n",
    "        return mock_responses.get(query[\"input\"], \"No information available.\")\n",
    "\n",
    "# Mock retrieval chain instance\n",
    "retrieval_chain = MockRetrievalChain()\n",
    "\n",
    "# Function to extract the last updated date from the log file\n",
    "def get_last_updated_date(log_file_path):\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        for line in reversed(lines):\n",
    "            match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', line)\n",
    "            if match:\n",
    "                return match.group(0)\n",
    "        return \"Unknown\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading log file: {e}\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Function to dynamically handle queries using the retrieval chain\n",
    "def ask_question(query):\n",
    "    try:\n",
    "        response = retrieval_chain.invoke({\"input\": query})\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ask_question: {e}\")\n",
    "        return \"An error occurred while retrieving the response.\"\n",
    "\n",
    "# Intent-to-query mapping for dynamic responses\n",
    "intent_query_mapping = {\n",
    "    \"Courses_and_Electives_Information\": \"What electives can I take for Cybersecurity?\",\n",
    "    \"Current_Student_Information\": \"What resources are available for current students?\",\n",
    "    \"Faculty_Information\": \"Who are the faculty members in the Data Science program?\",\n",
    "    \"OPT_and_CPT_Information\": \"What are the rules for OPT and CPT?\",\n",
    "    \"Prospective_Student_Information\": \"What should prospective students know about the UMBC Data Science program?\",\n",
    "    \"Research_Information\": \"What research opportunities are available in UMBC Data Science?\",\n",
    "}\n",
    "\n",
    "@app.post(\"/webhook\")\n",
    "async def webhook(request: Request):\n",
    "    try:\n",
    "        req = await request.json()\n",
    "        print(\"Incoming Request:\", json.dumps(req, indent=2))\n",
    "\n",
    "        # Extract the intent name\n",
    "        intent_name = req.get(\"queryResult\", {}).get(\"intent\", {}).get(\"displayName\", \"\")\n",
    "        print(f\"Triggered Intent: {intent_name}\")\n",
    "\n",
    "        # Handle the Default Welcome Intent\n",
    "        if intent_name == \"Default Welcome Intent\":\n",
    "            log_file_path = \"C:/Users/almas/OneDrive/Desktop/Fall 2024/webscraping/scraping_log.log\"\n",
    "            last_updated_date = get_last_updated_date(log_file_path)\n",
    "            response_text = (\n",
    "                f\"Hello! I was last updated on {last_updated_date}. \"\n",
    "                \"Information might have changed after this date. How can I assist you today?\"\n",
    "            )\n",
    "            return JSONResponse(content={\"fulfillmentText\": response_text})\n",
    "\n",
    "        # Handle intents dynamically using the retrieval chain\n",
    "        if intent_name in intent_query_mapping:\n",
    "            query = intent_query_mapping[intent_name]\n",
    "            response_text = ask_question(query)\n",
    "            return JSONResponse(content={\"fulfillmentText\": response_text})\n",
    "\n",
    "        # Fallback response for unknown intents\n",
    "        return JSONResponse(content={\"fulfillmentText\": \"I'm sorry, I couldn't process your request.\"})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in webhook: {e}\")\n",
    "        return JSONResponse(content={\"fulfillmentText\": \"An error occurred while processing your request.\"})\n",
    "\n",
    "# Apply nest_asyncio for Jupyter Notebook compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the FastAPI app\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e02530-94dc-47d5-8379-b7bad2ac5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the FastAPI app using nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "# # Run the FastAPI app\n",
    "# if __name__ == \"__main__\":\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9e00b-dfbd-4b28-8c64-6b33e4b15d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize LLM (e.g., OpenAI GPT-3)\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0.7)\n",
    "\n",
    "# Prompt template for generating responses\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are an assistant for the UMBC Data Science program. Answer questions accurately based on the provided context.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Retrieval logic (example)\n",
    "def retrieve_context(query):\n",
    "    # Replace with retrieval logic (e.g., querying a vector database)\n",
    "    # This is a placeholder example\n",
    "    context = {\n",
    "        \"Courses_and_Electives_Information\": \"Courses available include DATA 601, DATA 602, and DATA 603.\",\n",
    "        \"OPT_and_CPT_Information\": \"OPT allows international students to work after graduation. CPT is for internships during your studies.\",\n",
    "        \"Research_Information\": \"Research areas include AI, Cybersecurity, and Data Science.\"\n",
    "    }\n",
    "    return context.get(query, \"No specific context available.\")\n",
    "\n",
    "# Webhook endpoint\n",
    "@app.post(\"/webhook\")\n",
    "async def dialogflow_webhook(request: Request):\n",
    "    req = await request.json()\n",
    "\n",
    "    # Extract query and intent from Dialogflow\n",
    "    query = req.get(\"queryResult\", {}).get(\"queryText\", \"\")\n",
    "    intent_name = req.get(\"queryResult\", {}).get(\"intent\", {}).get(\"displayName\", \"\")\n",
    "\n",
    "    print(f\"Received query: {query}\")\n",
    "    print(f\"Detected Intent: {intent_name}\")\n",
    "\n",
    "    # Retrieve context based on the intent\n",
    "    context = retrieve_context(intent_name)\n",
    "\n",
    "    # Fill the prompt with the retrieved context and query\n",
    "    filled_prompt = prompt_template.format(context=context, question=query)\n",
    "\n",
    "    # Use LLM to generate a response\n",
    "    response = llm(filled_prompt)\n",
    "\n",
    "    # Return response to Dialogflow\n",
    "    return JSONResponse(content={\"fulfillmentText\": response.strip()})\n",
    "\n",
    "\n",
    "# Run the FastAPI server\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866b76c-064f-445c-b8ac-270e76f7069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "<script src=\"https://www.gstatic.com/dialogflow-console/fast/messenger/bootstrap.js?v=1\"></script>\n",
    "<df-messenger\n",
    "  intent=\"WELCOME\"\n",
    "  chat-title=\"RetreiverBot\"\n",
    "  agent-id=\"ab6278c2-9910-44c2-8601-b908c78f403c\"\n",
    "  language-code=\"en\"\n",
    "></df-messenger>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05007b1-21fe-4539-91c9-9823225ec642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (py311_env)",
   "language": "python",
   "name": "py311_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
